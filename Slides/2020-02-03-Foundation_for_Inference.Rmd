---
title: "Central Limit Theorem<br/>Null Hypothesis Testing<br/>Confidence Intervals"
author: Jason Bryer, Ph.D.
date: February 3, 2020
knit: (function(inputFile, encoding) { input.dir <- normalizePath(dirname(inputFile)); rmarkdown::render(input = inputFile, encoding = encoding, quiet=FALSE, output_file = paste0(input.dir,'/../docs/slides/', tools::file_path_sans_ext(basename(inputFile)), '.html')); })
output:
  ioslides_presentation:
    self_contained: true
    widescreen: true
    smaller: true
editor_options: 
  chunk_output_type: console
---
	
<style>
div.footnotes {
  position: absolute;
  bottom: 0;
  margin-bottom: 10px;
  width: 80%;
  font-size: 0.6em;
}
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>
<!-- Use for forced two column break: <p class="forceBreak"></p> -->

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
  $('slide:not(.backdrop):not(.title-slide)').append('<div class=\"footnotes\">');

  $('footnote').each(function(index) {
    var text  = $(this).html();
    var fnNum = (index+1).toString();
    $(this).html(fnNum.sup());

    var footnote   = fnNum + '. ' + text + '<br/>';
    var oldContent = $(this).parents('slide').children('div.footnotes').html();
    var newContent = oldContent + footnote;
    $(this).parents('slide').children('div.footnotes').html(newContent);
  });
});
</script>

<style>
.codefont pre {
    font-size: 12px;
    line-height: 10px;
}
</style>

<div class="notes">
Documentation on using ioslides is available here:
http://rmarkdown.rstudio.com/ioslides_presentation_format.html
Some slides are adopted (or copied) from OpenIntro: https://www.openintro.org/
</div>

```{r setup, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
set.seed(2112)
library(ggplot2)
library(openintro)
library(DATA606)
par(mar=c(2.5,1,2,1))

PlotDist <- function(alpha, from = -5, to = 5, n = 1000, filename = NULL,
    alternative = c("two.tailed", "greater", "lesser"), 
    distribution = c("normal", "t", "F", "chisq", "binomial"), 
    colour = "black", fill = "skyblue2",
    ...)
{
    alternative <- match.arg(alternative)
    alt.alpha <- switch(alternative, two.tailed = alpha/2, greater = alpha,
        lesser = alpha)
    MyDen <- switch(distribution, normal = dnorm, t = dt, F = df,
        chisq = dchisq, binomial = dbinom)
    MyDist <- switch(distribution, normal = qnorm, t = qt, F = qf,
        chisq = qchisq, binomial = qbinom)
    crit.lower <- MyDist(p = alt.alpha, lower.tail = TRUE, ...)
    crit.upper <- MyDist(p = alt.alpha, lower.tail = FALSE, ...)
    cord.x1 <- c(from, seq(from = from, to = crit.lower, length.out = 100),
        crit.lower)
    cord.y1 <- c(0, MyDen(x = seq(from = from, to = crit.lower,
        length.out = 100), ...), 0)
    cord.x2 <- c(crit.upper, seq(from = crit.upper, to = to,
        length.out = 100), to)
    cord.y2 <- c(0, MyDen(x = seq(from = crit.upper, to = to,
        length.out = 100), ...), 0)
    if (!is.null(filename)) pdf(file = filename)
    curve(MyDen(x, ...), from = from, to = to, n = n, col = colour,
        lty = 1, lwd = 2, ylab = "Density", xlab = "Values")
    if (!identical(alternative, "greater")) {
        polygon(x = cord.x1, y = cord.y1, col = fill)
    }
    if (!identical(alternative, "lesser")) {
        polygon(x = cord.x2, y = cord.y2, col = fill)
    }
    if (!is.null(filename)) dev.off()
}
```


## Population Distribution (Uniform)

```{r}
n <- 1e5
pop <- runif(n, 0, 1)
mean(pop)
```

<center>
```{r, echo=FALSE, fig.width=8,fig.height=3.5}
d <- density(pop)
h <- hist(pop, plot=FALSE)
hist(pop, main='Population Distribution', xlab="", freq=FALSE, 
     ylim=c(0, max(d$y, h$density)+.5), col=COL[1,2], border = "white", 
	 cex.main = 1.5, cex.axis = 1.5, cex.lab = 1.5)
lines(d, lwd=3)
```
</center>



## Random Sample (n=10)

```{r, fig.width=10, fig.height=5}
samp1 <- sample(pop, size=10)
mean(samp1)
```

<center>
```{r, fig.width=8,fig.height=3.5}
hist(samp1)
```
</center>

## Random Sample (n=30)

```{r, fig.width=8,fig.height=3.5}
samp2 <- sample(pop, size=30)
mean(samp2)
```

<center>
```{r, fig.width=8,fig.height=3.5}
hist(samp2)
```
</center>

## Lots of Random Samples

```{r, echo=TRUE}
M <- 1000
samples <- numeric(length=M)
for(i in seq_len(M)) {
	samples[i] <- mean(sample(pop, size=30))
}
head(samples, n=8)
```


## Sampling Distribution

<center>
```{r, fig.width=10, fig.height=5}
hist(samples)
```
</center>


## Central Limit Theorem (CLT)

Let $X_1$, $X_2$, ..., $X_n$ be independent, identically distributed random variables with mean $\mu$ and variance $\sigma^2$, both finite. Then for any constant $z$,

$$ \underset { n\rightarrow \infty  }{ lim } P\left( \frac { \bar { X } -\mu  }{ \sigma /\sqrt { n }  } \le z \right) =\Phi \left( z \right)  $$

where $\Phi$ is the cumulative distribution function (cdf) of the standard normal distribution.


## In other words...

The distribution of the sample mean is well approximated by a normal model:

$$ \bar { x } \sim N\left( mean=\mu ,SE=\frac { \sigma  }{ \sqrt { n }  }  \right)  $$

where SE represents the **standard error**, which is defined as the standard deviation of the sampling distribution. In most cases $\sigma$ is not known, so use $s$.


## CLT Shiny App

```{r, eval=FALSE}
shiny_demo('sampdist')
shiny_demo('CLT_mean')
```

## Standard Error

```{r}
samp2 <- sample(pop, size=30)
mean(samp2)
(samp2.se <- sd(samp2) / sqrt(length(samp2)))
```

## Confidence Interval

The confidence interval is then $\mu \pm CV \times SE$ where CV is the critical value. For a 95% confidence interval, the critical value is ~1.96 since

$$\int _{ -1.96 }^{ 1.96 }{ \frac { 1 }{ \sigma \sqrt { 2\pi  }  } { d }^{ -\frac { { \left( x-\mu  \right)  }^{ 2 } }{ 2{ \sigma  }^{ 2 } }  } } \approx 0.95$$

```{r}
qnorm(0.025) # Remember we need to consider the two tails, 2.5% to the left, 2.5% to the right.
```

```{r}
(samp2.ci <- c(mean(samp2) - 1.96 * samp2.se, mean(samp2) + 1.96 * samp2.se))
```


## Confidence Intervals (cont.)

We are 95% confident that the true population mean is between `r samp2.ci`. 

That is, if we were to take 100 random samples, we would expect at least 95% of those samples to have a mean within `r samp2.ci`.

```{r}
ci <- data.frame(mean=numeric(), min=numeric(), max=numeric())
for(i in seq_len(100)) {
	samp <- sample(pop, size=30)
	se <- sd(samp) / sqrt(length(samp))
	ci[i,] <- c(mean(samp),
				mean(samp) - 2 * se, 
				mean(samp) + 2 * se)
}
ci$sample <- 1:nrow(ci)
ci$sig <- ci$min < 0.5 & ci$max > 0.5
```


## Confidence Intervals 

```{r, eval=TRUE, fig.width=10, fig.height=4}
ggplot(ci, aes(x=min, xend=max, y=sample, yend=sample, color=sig)) + 
	geom_vline(xintercept=0.5) + 
	geom_segment() + xlab('CI') + ylab('') +
	scale_color_manual(values=c('TRUE'='grey', 'FALSE'='red'))
```



## Hypothesis Testing

* We start with a null hypothesis ($H_0$) that represents the status quo.
* We also have an alternative hypothesis ($H_A$) that represents our research question, i.e. what we???re testing for.
* We conduct a hypothesis test under the assumption that the null hypothesis is true, either via simulation or traditional methods based on the central limit theorem.
* If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative.


## Hypothesis Testing (using CI)

$H_0$: The mean of `samp2` = 0.5  
$H_A$: The mean of `samp2` $\ne$ 0.5

Using confidence intervals, if the *null* value is within the confidence interval, then we *fail* to reject the *null* hypothesis.

```{r}
(samp2.ci <- c(mean(samp2) - 2 * sd(samp2) / sqrt(length(samp2)),
			   mean(samp2) + 2 * sd(samp2) / sqrt(length(samp2))))
```

Since 0.5 fall within `r samp2.ci`, we *fail* to reject the null hypothesis.


## Hypothesis Testing (using *p*-values)

$$ \bar { x } \sim N\left( mean=0.49,SE=\frac { 0.27 }{ \sqrt { 30 } = 0.049 }  \right)  $$

$$ Z=\frac { \bar { x } -null }{ SE } =\frac { 0.49-0.50 }{ 0.049 } = -.204081633 $$

```{r}
pnorm(-.204) * 2
```

## Hypothesis Testing (using *p*-values)

<center>
```{r, fig.width=10, fig.height=5}
normalPlot(bounds=c(-.204, .204), tails=TRUE)
```
</center>


## Type I and II Errors

There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a decision about which might be true, but our choice might be incorrect.



|                    | fail to reject H<sub>0</sub> | reject H<sub>0</sub> |
|--------------------|:----------------------------:|:--------------------:|
| H<sub>0</sub> true |        	&#10004;            |  Type I Error        |
| H<sub>A</sub> true |     Type II Error            |      	&#10004;       |


<br /><br />

* Type I Error: **Rejecting** the null hypothesis when it is **true**.
* Type II Error: **Failing to reject** the null hypothesis when it is **false**.


## Hypothesis Test

If we again think of a hypothesis test as a criminal trial then it
makes sense to frame the verdict in terms of the null and
alternative hypotheses:

<p style="padding-left:150px">
H<sub>0</sub> : Defendant is innocent<br/>
H<sub>A</sub> : Defendant is guilty
</p>

Which type of error is being committed in the following
circumstances?

* Declaring the defendant innocent when they are actually guilty  
<center>Type 2 error</center>

* Declaring the defendant guilty when they are actually innocent  
<center>Type 1 error</center>

Which error do you think is the worse error to make? 


## Null Distribution {.flexbox .vcenter}

```{r, fig.width=6, fig.height=3.5}
(cv <- qnorm(0.05, mean=0, sd=1, lower.tail=FALSE))
PlotDist(alpha=0.05, distribution='normal', alternative='greater')
abline(v=cv, col='blue')
```

## Alternative Distribution {.flexbox .vcenter}

```{r, fig.width=6, fig.height=3.5}
cord.x1 <- c(-5, seq(from = -5, to = cv, length.out = 100), cv)
cord.y1 <- c(0, dnorm(mean=cv, x=seq(from=-5, to=cv, length.out = 100)), 0)
curve(dnorm(x, mean=cv), from = -5, to = 5, n = 1000, col = "black",
        lty = 1, lwd = 2, ylab = "Density", xlab = "Values")
polygon(x = cord.x1, y = cord.y1, col = 'lightgreen')
abline(v=cv, col='blue')
```

```{r}
pnorm(cv, mean=cv, lower.tail = FALSE)
```

## Another Example (mu = 2.5) {.flexbox .vcenter}

```{r}
mu <- 2.5
(cv <- qnorm(0.05, mean=0, sd=1, lower.tail=FALSE))
```

```{r, echo=FALSE, fig.width=3.5, fig.height=3.5, fig.show='hold'}
pv <- pnorm(mu, mean=0, sd=1, lower.tail=FALSE)

PlotDist(alpha=pv, distribution='normal', alternative='greater')
abline(v=mu, col='blue')
title('Null Distribution')

cord.x1 <- c(-5, seq(from = -5, to = cv, length.out = 100), cv)
cord.y1 <- c(0, dnorm(mean=mu, x=seq(from=-5, to=cv, length.out = 100)), 0)
curve(dnorm(x, mean=mu), from = -5, to = 5, n = 1000, col = "black",
        lty = 1, lwd = 2, ylab = "Density", xlab = "Values")
polygon(x = cord.x1, y = cord.y1, col='lightgreen')
abline(v=mu, col='blue')
title('Alternative Distribution')
```

## Numeric Values

Type I Error

```{r}
pnorm(mu, mean=0, sd=1, lower.tail=FALSE)
```

Type II Error

```{r}
pnorm(cv, mean=mu, lower.tail = TRUE)
```

## Shiny Application

Visualizing Type I and Type II errors: [https://bcdudek.net/betaprob/](https://bcdudek.net/betaprob/)

## Why p < 0.05?

Check out this page: https://www.openintro.org/stat/why05.php

See also:

Kelly M. [*Emily Dickinson and monkeys on the stair Or: What is the significance of the 5% significance level?*](http://www.acsu.buffalo.edu/~grant/5pcMarkKelley.pdf) Significance 10:5. 2013.


## Statistical vs. Practical Significance

* Real differences between the point estimate and null value are easier to detect with larger samples.
* However, very large samples will result in statistical significance even for tiny differences between the sample mean and the null value (effect size), even when the difference is not practically significant.
* This is especially important to research: if we conduct a study, we want to focus on finding meaningful results (we want observed differences to be real, but also large enough to matter).
* The role of a statistician is not just in the analysis of data, but also in planning and design of a study.

## Bootstrapping

* First introduced by Efron (1979) in [*Bootstrap Methods: Another Look at the Jackknife*](https://projecteuclid.org/euclid.aos/1176344552).
* Estimates confidence of statistics by resampling *with* replacement.
* The *bootstrap sample* provides an estimate of the sampling distribution.
* The `boot` R package provides a framework for doing bootstrapping: https://www.statmethods.net/advstats/bootstrapping.html

## Bootstrapping Example (Population)

Define our population with a uniform distribution.

```{r}
n <- 1e5
pop <- runif(n, 0, 1)
mean(pop)
```

<center>
```{r, echo=FALSE, fig.width=8,fig.height=3.5}
d <- density(pop)
h <- hist(pop, plot=FALSE)
hist(pop, main='Population Distribution', xlab="", freq=FALSE, 
     ylim=c(0, max(d$y, h$density)+.5), col=COL[1,2], border = "white", 
	 cex.main = 1.5, cex.axis = 1.5, cex.lab = 1.5)
lines(d, lwd=3)
```
</center>

## Bootstrapping Example (Sample)

We observe one random sample from the population.

```{r}
samp1 <- sample(pop, size = 50)
```

```{r, echo=FALSE, fig.width=8,fig.height=3.5}
d <- density(samp1)
h <- hist(samp1, plot=FALSE)
hist(samp1, main='Distribution of Sample', xlab="", freq=FALSE, 
     ylim=c(0, max(d$y, h$density)+.5), col=COL[1,2], border = "white", 
	 cex.main = 1.5, cex.axis = 1.5, cex.lab = 1.5)
lines(d, lwd=3)
```

## Bootsrapping Example (Estimate)

```{r}
boot.samples <- numeric(1000) # 1,000 bootstrap samples
for(i in seq_along(boot.samples)) { 
	tmp <- sample(samp1, size = length(samp1), replace = TRUE)
	boot.samples[i] <- mean(tmp)
}
head(boot.samples)
```

## Bootsrapping Example (Distribution)

```{r}
d <- density(boot.samples)
h <- hist(boot.samples, plot=FALSE)
hist(boot.samples, main='Bootstrap Distribution', xlab="", freq=FALSE, 
     ylim=c(0, max(d$y, h$density)+.5), col=COL[1,2], border = "white", 
	 cex.main = 1.5, cex.axis = 1.5, cex.lab = 1.5)
lines(d, lwd=3)
```

## 95% confidence interval

```{r}
c(mean(boot.samples) - 1.96 * sd(boot.samples), 
  mean(boot.samples) + 1.96 * sd(boot.samples))
```

## Bootstrapping is not just for means!

```{r}
boot.samples.median <- numeric(1000) # 1,000 bootstrap samples
for(i in seq_along(boot.samples.median)) { 
	tmp <- sample(samp1, size = length(samp1), replace = TRUE)
	boot.samples.median[i] <- median(tmp) # NOTICE WE ARE NOW USING THE median FUNCTION!
}
head(boot.samples.median)
```

95% confidence interval for the median

```{r}
c(mean(boot.samples.median) - 1.96 * sd(boot.samples.median), 
  mean(boot.samples.median) + 1.96 * sd(boot.samples.median))
```

# Inference for Categorical Data

## Example

Two scientists want to know if a certain drug is effective against high blood pressure. The first scientist wants to give the drug to 1,000 people with high blood pressure and see how many of them experience lower blood pressure levels. The second scientist wants to give the drug to 500 people with high blood pressure, and not give the drug to another 500 people with high blood pressure, and see how many in both groups experience lower blood pressure levels. Which is the better way to test this drug?

>- **500 get the drug, 500 don't**

## Survey of Americans

The GSS asks the same question, below is the distribution of responses from the 2010 survey:

Response                   | n
---------------------------|------
All 1000 get the drug      | 99 
500 get the drug 500 don't | 571
**Total**                  | **670**

## Parameter of Interest

* Parameter of interest: Proportion of *all* Americans who have good intuition about experimental design.  
$$p(population\; proportion)$$

* Point estimate: Proportion of *sampled* Americans who have good
intuition about experimental design.  
$$\hat{p}(sample\; proportion)$$

## Inference for a proportion

What percent of all Americans have good intuition about experimental design (i.e. would answer "500 get the drug 500 don't?"

* Using a confidence interval
$$point\; estimate \pm ME$$

* We know that ME = critical value x standard error of the point estimate.
$$SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}$$

##  Central limit theoreom for proportions

Sample proportions will be nearly normally distributed with mean equal to the population mean, *p*, and standard error equal to $\sqrt{\frac{p(1-p)}{n}}$.

$$\hat { p } \sim N\left( mean=p,SE=\sqrt { \frac { p(1-p) }{ n }  }  \right) $$

This is true given the following conditions:

* independent observations
* at least 10 successes and 10 failures

## Back to the Survey

* 571 out of 670 (85%) of Americans answered the question on experimental design correctly.
* Estimate (using a 95% confidence interval) the proportion of all Americans who have good intuition about experimental design?

Given: $n = 670$, $\hat{p} = 0.85$.

Conditions:

1. Independence: The sample is random, and 670 < 10% of all Americans, therefore we can assume that one respondent's response is independent of another.

2. Success-failure: 571 people answered correctly (successes) and 99 answered incorrectly (failures), both are greater than 10.

## Calculating Confidence Interval

Given: $n = 670$, $\hat{p} = 0.85$.

$$0.85 \pm 1.96 \sqrt{\frac{0.85 \times 0.15}{670}} = \left(0.82,\; 0.88\right)$$

We are 95% confidence the true proportion of Americans that have a good intuition about experimental designs is betwee 82% and 88%.

## How many should we sample?

Suppose you want a 3% margin of error, how many people would you have to survey?

Use $\hat{p} = 0.5$

* If you don't know any better, 50-50 is a good guess
* $\hat{p} = 0.5$ gives the most conservative estimate - highest possible sample size

$$0.03 = 1.96 \times \sqrt{\frac{0.5 \times 0.5}{n}}$$
$$0.03^2 = 1.96^2 \times \frac{0.5 \times 0.5}{n}$$
$$n \approx 1,068$$

## Example: Two Proportions

Scientists predict that global warming may have big effects on the polar regions within the next 100 years. One of the possible effects is that the northern ice cap may completely melt. Would this bother you a great deal, some, a little, or not at all if it actually happened?

Response     | GSS | Duke
-------------|----:|-----:
A great deal | 454 |  69
Some         | 124 |  40
A little     |  52 |   4
Not at all   |  50 |   2
Total        | 680 | 105

## Parameter and Point Estimate

Parameter of interest: Difference between the proportions of *all* Duke students and *all* Americans who would be bothered a great deal by the northern ice cap completely melting.

$$p_{Duke} - p_{US}$$

Point estimate: Difference between the proportions of *sampled* Duke students and *sampled* Americans who would be bothered a great deal by the northern ice cap completely melting.

$$\hat{p}_{Duke} - \hat{p}_{US}$$

## Everything else is the same...

* CI: $point\; estimate \pm margin\; of\; error$
* HT: $Z = \frac{point\; estimate - null\; value}{SE}$

Standard error of the difference between two sample proportions

$$SE_{\hat{p}_1 - \hat{p}_2} = \sqrt{ \frac{p_1\left(1 - p_1\right)}{n_1} + \frac{p_2\left(1 - p_2\right)}{n_2} }$$

Conditions:

1. Independence within groups: The US group is sampled randomly and we're assuming that the Duke group represents a random sample as well. $n_{Duke} < 10\%$ of all Duke students and $680 < 10\%$ of all Americans.
2. Independence between groups: The sampled Duke students and the US residents are independent of each other.
3. Success-failure: At least 10 observed successes and 10 observed failures in the two groups.

## 

Construct a 95% confidence interval for the difference between the proportions of Duke students and Americans who would be bothered a great deal by the melting of the northern ice cap ($p_{Duke} - p_{US}$).

Data             |  Duke |  US
-----------------|------:|-----:
A great deal     |   69  | 454
Not a great deal |   36  | 226
Total            |  105  | 680
$\hat{p}$        | 0.657 | 0.668

$$ \left(\hat{p}_{Duke} - \hat{p}_{US}\right) \pm z* \times \sqrt{ \frac{p_{Duke}\left(1 - p_{Duke}\right)}{n_{Duke}} + \frac{p_{US}\left(1 - p_{US}\right)}{n_{US}} } $$

$$(0.657 - 0.668) \pm 1.96 \times \sqrt{\frac{0.657 \times 0.343}{105} + \frac{0.668 \times 0.332}{680}} = \left(-0.108,\; 0.086\right)$$


# Inference for Numerical Data

```{r setup2, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
set.seed(2112)
library(ggplot2)
library(openintro)
library(DATA606)
library(reshape2)
library(psych)
library(granova)
library(tidyverse)
library(latex2exp)

par(mar=c(2.5,1,2,1))

```


## Independence Between Groups

Assume we have a population of 100,000 where groups A and B are independent with $p_A = .55$ and $p_B = .6$ and $n_A = 99,000$ (99% of the population) and $n_B = 1,000$ (1% of the population). We can sample from the population (that includes groups A and B) and from group B of sample sizes of 1,000 and 100, respectively. We can also calculate $\hat{p}$ for group A independent of B.

```{r}
propA <- .55    # Proportion for group A
propB <- .6     # Proportion for group B
pop.n <- 100000 # Population size
sampleA.n <- 1000
sampleB.n <- 100

pop <- data.frame(
	group = c(rep('A', pop.n * 0.99),
			  rep('B', pop.n * 0.01) ),
	response = c(
		sample(c(1,0), size = pop.n * 0.99, prob = c(propA, 1 - propA), 
			   replace = TRUE),
		sample(c(1,0), size = pop.n * 0.01, prob = c(propB, 1 - propB), 
			   replace = TRUE) )
)

sampA <- pop[sample(nrow(pop), size = sampleA.n),]
sampB <- pop[sample(which(pop$group == 'B'), size = sampleB.n),]
```

## Independence Between Groups (cont.)

$\hat{p}$ for the population sample

```{r}
mean(sampA$response)

```

$\hat{p}$ for the population sample, excluding group B 

```{r}
mean(sampA[sampA$group == 'A',]$response)
```

$\hat{p}$ for group B sample

```{r}
mean(sampB$response)
```

## Independence Between Groups (cont.)

```{r, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
propA <- .55 # Proportion for group A
propB <- .6 # Proportion for group B
pop.n <- 100000 # Population size
sampleA.n <- 1000
sampleB.n <- 1000

replications <- 10 

results <- data.frame(
	percent = rep(seq(0.01, 0.5, 0.01), each = replications),
	p_hat_pop = NA_real_,
	p_hat_popA = NA_real_,
	p_hat_popB = NA_real_,
	p_hat_A = NA_real_,
	p_hat_A_not_B = NA_real_,
	p_hat_B = NA_real_
)

for(i in seq_len(nrow(results))) {
	A.n <- pop.n * (1 - results[i,]$percent)
	B.n <- pop.n * results[i,]$percent
	
	pop <- data.frame(
		group = c(rep('A', A.n),
				  rep('B', B.n) ),
		response = c(
			sample(c(1,0), 
				   size = A.n, 
				   prob = c(propA, 1 - propA), 
				   replace = TRUE),
			sample(c(1,0), 
				   size = B.n, 
				   prob = c(propB, 1 - propB), 
				   replace = TRUE) )
	)
	
	tmp <- aggregate(pop$response, by = list(pop$group), FUN = mean)
	results[i,]$p_hat_pop <- mean(pop$response)
	results[i,]$p_hat_popA <- tmp[1,]$x
	results[i,]$p_hat_popB <- tmp[2,]$x
	
	sampA <- pop[sample(nrow(pop), size = sampleA.n),]
	sampB <- pop[sample(which(pop$group == 'B'), size = sampleB.n),]
	
	results[i,]$p_hat_A <- mean(sampA$response)
	results[i,]$p_hat_A_not_B <- mean(sampA[sampA$group == 'A',]$response)
	results[i,]$p_hat_B <- mean(sampB$response)
}

results.melt <- melt(results[,c(1,5,6,7)], 
					 id.vars = 'percent', 
					 value.name = 'p_hat')
```

```{r, echo=FALSE, fig.width=10, fig.height=5.5}
ggplot(results.melt, aes(x = percent, y = p_hat, color = variable)) + 
	geom_hline(yintercept = propA) +
	geom_hline(yintercept = propB) + 
	geom_point(alpha = 0.2) + 
	geom_smooth(se = TRUE, method = 'loess') +
	scale_color_brewer('', palette = 2, type = 'qual') +
	xlab('Size of group B as a percentage of population') +
	ylab(TeX('$\\hat{p}$'))
```


## High School & Beyond Survey  {.flexbox .vcenter}

`r nrow(hsb2)` randomly selected students completed the reading and writing test of the High School and Beyond survey. The results appear to the right. Does there appear to be a difference?

```{r, fig.width=5, fig.height=3.7, eval=TRUE}
data(hsb2) # in openintro package
hsb2.melt <- melt(hsb2[,c('id','read', 'write')], id='id')
ggplot(hsb2.melt, aes(x=variable, y=value)) + 	geom_boxplot() + 
	geom_point(alpha=0.2, color='blue') + xlab('Test') + ylab('Score')
```


## High School & Beyond Survey  {.flexbox .vcenter}

```{r}
head(hsb2)
```

Are the reading and writing scores of each student independent of each other?


## Analyzing Paired Data  {.flexbox .vcenter}

* When two sets of observations are not independent, they are said to be paired.
* To analyze these type of data, we often look at the difference.

```{r, fig.width=6, fig.height=4}
hsb2$diff <- hsb2$read - hsb2$write
head(hsb2$diff)
```

```{r, fig.width=6, fig.height=3}
hist(hsb2$diff)
```


## Setting the Hypothesis

What are the hypothesis for testing if there is a difference between the average reading and writing scores?

$H_0$: There is no difference between the average reading and writing scores.

$$\mu_{diff} = 0$$

$H_A$: There is a difference between the average reading and writing score.

$$\mu_{diff} \ne 0$$

## Nothing new here...

* The analysis is no different that what we have done before.
* We have data from one sample: differences.
* We are testing to see if the average difference is different that 0.

## Calculating the test-statistic and the p-value {.flexbox .vcenter}

The observed average difference between the two scores is `r mean(hsb2$diff)` points and the standard deviation of the difference is `r sd(hsb2$diff)` points. Do these data provide confincing evidence of a difference between the average scores ont eh two exams (use $\alpha = 0.05$)?

```{r, fig.width=6, fig.height=3.5, echo=FALSE}
meanDiff <- mean(hsb2$diff)
sdDiff <- sd(hsb2$diff)
normalPlot(mean=0, bounds=c(-1 * abs(meanDiff), abs(meanDiff)), tails=TRUE)
```

## Calculating the test-statistic and the p-value {.flexbox .vcenter}

$$Z = \frac{-0.545 - 0}{ \frac{8.887}{\sqrt{200}} } = \frac{-0.545}{0.628} = -0.87$$
$$p-value = 0.1949 \times 2 = 0.3898$$

Since p-value > 0.05, we fail to reject the null hypothesis. That is, the data do not provide evidence that there is a statistically significant difference between the average reading and writing scores.

```{r}
2 * pnorm(mean(hsb2$diff), mean=0, sd=sd(hsb2$diff)/sqrt(nrow(hsb2)))
```

## Interpretation of the p-value

The probability of obtaining a random sample of 200 students where the average difference between the reading and writing scores is at least 0.545 (in either direction), if in fact the true average difference between the score is 0, is 38%.

## Calculating 95% Confidence Interval

$$-0.545\pm 1.96\frac { 8.887 }{ \sqrt { 200 }  } =-0.545\pm 1.96\times 0.628=(-1.775, 0.685)$$

Note that the confidence interval spans zero!

## SAT Scores by Gender

```{r}
data(sat)
head(sat)
```

```{r, echo=FALSE, results='hide', warning=FALSE}
sat$Math.SAT <- as.integer(sat$Math.SAT)
sat <- sat[complete.cases(sat),]
```

Is there a difference in math scores between males and females?

## SAT Scores by Gender {.flexbox .vcenter}


```{r}
describeBy(sat$Math.SAT, group=sat$Sex, mat=TRUE, skew=FALSE)[,c(2,4:7)]
```

```{r, fig.width=4.5, fig.height=3}
ggplot(sat, aes(x=Sex, y=Math.SAT)) + geom_boxplot()
```

## Distributions  {.flexbox .vcenter}


```{r, fig.width=6, fig.height=4}
ggplot(sat, aes(x=Math.SAT)) + geom_histogram(binwidth=50) + facet_wrap(~ Sex)
```

## 95% Confidence Interval

We wish to calculate a 95% confidence interval for the average difference between SAT scores for males and females.

Assumptions:

1. Independence within groups.
2. Independence between groups.
3. Sample size/skew

## Confidence Interval for Difference Between Two Means

* All confidence intervals have the same form: point estimate ?? ME
* And all ME = critical value ?? SE of point estimate
* In this case the point estimate is $\bar{x}_1 - \bar{x}_2$
Since the sample sizes are large enough, the critical value is z*
So the only new concept is the standard error of the difference between two means...

Standard error of the difference between two sample means

$$ SE_{ (\bar { x } _{ 1 }-\bar { x } _{ 2 }) }=\sqrt { \frac { { s }_{ 1 }^{ 2 } }{ { n }_{ 1 } } +\frac { { s }_{ 2 }^{ 2 } }{ { n }_{ 2 } }  }  $$


## Confidence Interval for Difference in SAT Scores

$$ SE_{ (\bar { x } _{ 1 }-\bar { x } _{ 2 }) }=\sqrt { \frac { { s }_{ M }^{ 2 } }{ { n }_{ M } } + \frac { { s }_{ F }^{ 2 } }{ { n }_{ F } }  } = \sqrt { \frac { 90.4 }{ 80 } +\frac { 103.7 }{ 82 }  } =1.55 $$
 
## Student's *t*-Distribution {.columns-2}

What if you want to compare the quality of one batch of Guinness beer to the next?

* Sample sizes necessarily need to be small.
* The CLT states that the sampling distribution approximates normal as n -> Infinity
* Need an alternative to the normal distribution.
* The *t* distribution was developed by William Gosset (under the pseudonym *student*) to estimate means when the sample size is small.

Confidence interval is estamated using

$$\overline { x } \pm { t }_{ df }^{ * }SE$$

Where *df* is the degrees of freedom (*df* = *n* -1)


<p class="forceBreak"></p>

![](images/William_Sealy_Gosset.jpg)


## *t*-Distributions {.centered}

```{r, echo=FALSE}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)

degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")

plot(x, hx, type="l", lty=2, xlab="x value",
  ylab="Density", main="Comparison of t Distributions")

for (i in 1:4){
  lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}

legend("topright", inset=.05, title="Distributions", labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
```

## *t*-test in R {.columns-2}

The `pt` and `qt` will give you the *p*-value and critical value from the *t*-distribution, respectively.

Critical value for p = 0.05, degrees of freedom = 10

```{r}
qt(0.025, df = 10)
```

p-value for a critical value of 2, degrees of freedom = 10

```{r}
pt(2, df=10)
```

<p class="forceBreak"></p>

The `t.test` function will calculate a null hypothesis test using the *t*-distribution.

```{r}
t.test(Math.SAT ~ Sex, data = sat)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
require(ggplot2)
require(gdata)
require(psych)
require(granova)
require(granovaGG)
require(lattice)
data(singer)
data(rat)
hand <- read.csv('../course_data/Hand_washing.csv')
```




